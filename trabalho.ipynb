{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/hygo2025/cdd-projeto-final/blob/main/trabalho.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Install java\n",
        "!apt install openjdk-21-jdk"
      ],
      "metadata": {
        "collapsed": true,
        "id": "mE1noAqF9PdU"
      },
      "id": "mE1noAqF9PdU",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "\n",
        "!wget \"https://repo1.maven.org/maven2/org/apache/hadoop/hadoop-aws/3.3.4/hadoop-aws-3.3.4.jar\" -P \"spark/lib-jars/\"\n",
        "!wget \"https://repo1.maven.org/maven2/com/amazonaws/aws-java-sdk-bundle/1.12.699/aws-java-sdk-bundle-1.12.699.jar\" -P \"spark/lib-jars/\"\n",
        "!wget \"https://repo1.maven.org/maven2/com/amazonaws/aws-java-sdk/1.12.699/aws-java-sdk-1.12.699.jar\" -P \"spark/lib-jars/\"\n",
        "!wget \"https://repo1.maven.org/maven2/com/amazonaws/aws-java-sdk-core/1.12.699/aws-java-sdk-core-1.12.699.jar\" -P \"spark/lib-jars/\"\n",
        "!wget \"https://repo1.maven.org/maven2/com/amazonaws/aws-java-sdk-s3/1.12.699/aws-java-sdk-s3-1.12.699.jar\" -P \"spark/lib-jars/\""
      ],
      "metadata": {
        "collapsed": true,
        "id": "kxGSoMTz_9Ce"
      },
      "id": "kxGSoMTz_9Ce",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!java --version"
      ],
      "metadata": {
        "id": "pTK3BZ8ZVfcZ"
      },
      "id": "pTK3BZ8ZVfcZ",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "try:  # When on Google Colab, clone the repository to download any necessary cache.\n",
        "    import google.colab\n",
        "    repo_path = 'cdd'\n",
        "    !git -C $repo_path pull origin || git clone https://github.com/hygo2025/cdd-projeto-final.git $repo_path\n",
        "except:\n",
        "    repo_path = '.'  # Use the local path if not on Google Colab"
      ],
      "metadata": {
        "id": "elmgBR9pVh0g"
      },
      "id": "elmgBR9pVh0g",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "!pip install -r cdd/requirements.txt"
      ],
      "metadata": {
        "id": "mTfu6fnAVkYc"
      },
      "id": "mTfu6fnAVkYc",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "import pkg_resources\n",
        "!pip install -e $repo_path"
      ],
      "metadata": {
        "id": "GAgHJ4f8VtHP"
      },
      "id": "GAgHJ4f8VtHP",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# configurar as variáveis de ambiente\n",
        "import os\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-21-openjdk-amd64\"\n",
        "\n",
        "os.environ[\"SPARK_DRIVER_EXTRACLASSPATH\"] = \"/content/spark/lib-jars/*\"\n",
        "os.environ[\"SPARK_EXECUTOR_EXTRACLASSPATH\"] = \"/content/spark/lib-jars/*\"\n",
        "os.environ[\"SPARK_SERIALIZER\"] = \"org.apache.spark.serializer.KryoSerializer\"\n",
        "os.environ[\"ENV\"] = \"dev\"\n",
        "os.environ[\"PYTHONUNBUFFERED\"] = \"1\"\n",
        "os.environ[\"SPARK_EXECUTOR_MEMORY\"] = \"8g\"\n",
        "os.environ[\"SPARK_DRIVER_MEMORY\"] = \"8g\"\n",
        "os.environ[\"SPARK_MEMORY_FRACTION\"] = \"0.8\"\n",
        "\n",
        "\n",
        "os.environ[\"DOWNLOAD_FOLDER\"] = \"/content/datasets\""
      ],
      "metadata": {
        "id": "H939YnnX-A3D"
      },
      "id": "H939YnnX-A3D",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%load_ext autoreload\n",
        "%autoreload 2"
      ],
      "metadata": {
        "id": "V4DTuR9P--H2"
      },
      "id": "V4DTuR9P--H2",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "try:  # When on Google Colab, clone the repository to download any necessary cache.\n",
        "    import google.colab\n",
        "    repo_path = 'cdd'\n",
        "    !git -C $repo_path pull origin || git clone https://github.com/hygo2025/cdd-projeto-final.git $repo_path\n",
        "except:\n",
        "    repo_path = '.'  # Use the local path if not on Google Colab"
      ],
      "metadata": {
        "collapsed": true,
        "id": "4xGPDpcYBHmU"
      },
      "id": "4xGPDpcYBHmU",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import sys\n",
        "import os\n",
        "import pkg_resources\n",
        "\n",
        "if repo_path not in sys.path:\n",
        "    sys.path.append(repo_path)"
      ],
      "metadata": {
        "id": "xPE1A11QBnM9"
      },
      "id": "xPE1A11QBnM9",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import cdd"
      ],
      "metadata": {
        "id": "n6dYXTkxFlEF"
      },
      "id": "n6dYXTkxFlEF",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "full_results = pd.DataFrame()"
      ],
      "metadata": {
        "id": "oib-FQl_cw0W"
      },
      "id": "oib-FQl_cw0W",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from src.sar_model import SarModel\n",
        "from src.utils.enums import MovieLensDataset, SimilarityType\n",
        "\n",
        "def evaluate_sar():\n",
        "    top_k = 10\n",
        "    validate_size = 0.25\n",
        "    time_decay_coefficient = 30\n",
        "    seed = 42\n",
        "    df_results = pd.DataFrame()\n",
        "\n",
        "\n",
        "    # Iterando sobre todas as similaridades definidas na enumeração\n",
        "    for similarity in SimilarityType:\n",
        "        print(f\"\\nTestando similaridade: {similarity.value}\")\n",
        "\n",
        "        # Instanciando o modelo com a similaridade atual\n",
        "        sar_model = SarModel(\n",
        "            dataset=MovieLensDataset.ML_1M,\n",
        "            top_k=top_k,\n",
        "            validate_size=validate_size,\n",
        "            time_decay_coefficient=time_decay_coefficient,\n",
        "            similarity_type=similarity,\n",
        "            seed=seed\n",
        "        )\n",
        "\n",
        "        # Avaliando o modelo\n",
        "        result = sar_model.evaluate()\n",
        "\n",
        "        result['version'] = f\"similarity_{similarity.value}_top_k_{top_k}_validate_size_{validate_size}_time_decay_coefficient_{time_decay_coefficient}\"\n",
        "        result['algorithm'] = 'sar'\n",
        "\n",
        "        df_results = pd.concat([df_results, result], ignore_index=True)\n",
        "\n",
        "        print(f\"Resultado para similaridade {similarity.value}:\")\n",
        "        print(result)\n",
        "    return df_results\n"
      ],
      "metadata": {
        "id": "H1K9GdD8QYrk"
      },
      "id": "H1K9GdD8QYrk",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sar_results = evaluate_sar()"
      ],
      "metadata": {
        "collapsed": true,
        "id": "211XIAhvV6jM"
      },
      "id": "211XIAhvV6jM",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "full_results = pd.concat([full_results, sar_results], ignore_index=True)\n",
        "full_results.to_csv('01_sar.csv', index=False)"
      ],
      "metadata": {
        "id": "XthPbpk3c15N"
      },
      "id": "XthPbpk3c15N",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import itertools\n",
        "import pandas as pd\n",
        "from src.utils.enums import MovieLensDataset\n",
        "from src.als_model import SparkAlsModel\n",
        "\n",
        "def evaluate_als(spark):\n",
        "    # Definindo os grids de parâmetros\n",
        "    max_iter_options = [20]\n",
        "    rank_options = [10, 20, 30, 40]\n",
        "    reg_param_options = [0.05]\n",
        "    alpha_options = [0.1]\n",
        "\n",
        "    # Parâmetros fixos\n",
        "    validate_size = 0.25\n",
        "    top_k = 10\n",
        "    seed = 42\n",
        "    dataset = MovieLensDataset.ML_1M\n",
        "\n",
        "    # Cria um DataFrame vazio para armazenar os resultados\n",
        "    df_results = pd.DataFrame()\n",
        "\n",
        "    # Itera por todas as combinações de parâmetros\n",
        "    for max_iter, rank, reg_param, alpha in itertools.product(\n",
        "            max_iter_options, rank_options, reg_param_options, alpha_options\n",
        "    ):\n",
        "        print(f\"\\nTestando: maxIter={max_iter}, rank={rank}, regParam={reg_param}, alpha={alpha}\")\n",
        "\n",
        "        # Instanciando o modelo ALS com a configuração atual\n",
        "        als_model = SparkAlsModel(\n",
        "            spark=spark,\n",
        "            dataset=dataset,\n",
        "            max_iter=max_iter,\n",
        "            rank=rank,\n",
        "            reg_param=reg_param,\n",
        "            alpha=alpha,\n",
        "            validate_size=validate_size,\n",
        "            top_k=top_k,\n",
        "            seed=seed\n",
        "        )\n",
        "\n",
        "\n",
        "        # Avaliando o modelo\n",
        "        result = als_model.evaluate()\n",
        "\n",
        "        # Adiciona as informações desejadas\n",
        "        result['algorithm'] = 'als'\n",
        "        result['version'] = f\"max_iter_{max_iter}_rank_{rank}_reg_param_{reg_param}_alpha_{alpha}\"\n",
        "\n",
        "        df_results = pd.concat([df_results, result], ignore_index=True)\n",
        "        print(f\"Resultado: {result}\")\n",
        "\n",
        "\n",
        "    return df_results"
      ],
      "metadata": {
        "id": "BjUzjxEKZAKE"
      },
      "id": "BjUzjxEKZAKE",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from src.utils.spark_session_utils import create_spark_session\n",
        "\n",
        "spark = create_spark_session(\"ALS\")\n",
        "als_result = evaluate_als(spark)"
      ],
      "metadata": {
        "id": "HX3T0HZwcNDJ"
      },
      "id": "HX3T0HZwcNDJ",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "full_results = pd.concat([full_results, als_result], ignore_index=True)\n",
        "full_results.to_csv('02_sar_als.csv', index=False)"
      ],
      "metadata": {
        "id": "tTfNcFiBvYKo"
      },
      "id": "tTfNcFiBvYKo",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import itertools\n",
        "import pandas as pd\n",
        "from src.utils.enums import MovieLensDataset\n",
        "from src.ncf_model import NcfModel\n",
        "\n",
        "def evaluate_ncf():\n",
        "    # Parâmetros a serem testados\n",
        "    n_factors_options = [4]\n",
        "    batch_size_options = [256, 512]\n",
        "    lr_options = [1e-3]\n",
        "    epochs_options = [10, 15]\n",
        "\n",
        "    # Parâmetros fixos\n",
        "    layer_sizes = [16, 8, 4]\n",
        "    top_k = 10\n",
        "    test_size = 0.25\n",
        "    seed = 42\n",
        "    dataset = MovieLensDataset.ML_1M\n",
        "\n",
        "    # Cria um DataFrame vazio para armazenar os resultados\n",
        "    df_results = pd.DataFrame()\n",
        "\n",
        "    # Itera por todas as combinações de parâmetros\n",
        "    for n_factors, batch_size, lr, epochs in itertools.product(\n",
        "        n_factors_options, batch_size_options, lr_options, epochs_options\n",
        "    ):\n",
        "        print(f\"\\nTestando: n_factors={n_factors}, batch_size={batch_size}, lr={lr}, epochs={epochs}\")\n",
        "\n",
        "        # Instancia o modelo NCF com a configuração atual\n",
        "        model = NcfModel(\n",
        "            dataset=dataset,\n",
        "            n_factors=n_factors,\n",
        "            batch_size=batch_size,\n",
        "            lr=lr,\n",
        "            layer_sizes=layer_sizes,\n",
        "            epochs=epochs,\n",
        "            top_k=top_k,\n",
        "            test_size=test_size,\n",
        "            seed=seed\n",
        "        )\n",
        "\n",
        "        # Avalia o modelo\n",
        "        result = model.evaluate()\n",
        "\n",
        "        # Adiciona informações de identificação\n",
        "        result['algorithm'] = 'ncf'\n",
        "        result['version'] = f\"n_factors_{n_factors}_batch_size_{batch_size}_lr_{lr}_epochs_{epochs}\"\n",
        "\n",
        "        df_results = pd.concat([df_results, result], ignore_index=True)\n",
        "\n",
        "        print(f\"Resultado: {result}\")\n",
        "\n",
        "    return df_results"
      ],
      "metadata": {
        "id": "YxsLihPdpeCk"
      },
      "id": "YxsLihPdpeCk",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ncf_result = evaluate_ncf()"
      ],
      "metadata": {
        "id": "YhQkGyPRuAGH"
      },
      "id": "YhQkGyPRuAGH",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "full_results = pd.concat([full_results, ncf_result], ignore_index=True)\n",
        "full_results.to_csv('03_sar_als_ncf.csv', index=False)"
      ],
      "metadata": {
        "id": "6A408Ba9t_H5"
      },
      "id": "6A408Ba9t_H5",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import itertools\n",
        "import pandas as pd\n",
        "from src.utils.enums import MovieLensDataset\n",
        "from src.bivae_model import BivaeModel\n",
        "\n",
        "def evaluate_bivae():\n",
        "    # Parâmetros a serem testados\n",
        "    latent_dim_options = [50, 100]\n",
        "    epochs_options = [300, 500]\n",
        "    lr_options = [0.001]\n",
        "\n",
        "    # Parâmetros fixos\n",
        "    top_k = 10\n",
        "    batch_size = 1024\n",
        "    encoder_dims = [100]\n",
        "    act_func = 'tanh'\n",
        "    likelihood = 'pois'\n",
        "    test_size = 0.25\n",
        "    seed = 42\n",
        "    dataset = MovieLensDataset.ML_1M\n",
        "\n",
        "    # Cria um DataFrame vazio para armazenar os resultados\n",
        "    df_results = pd.DataFrame()\n",
        "\n",
        "    # Itera por todas as combinações de parâmetros\n",
        "    for latent_dim, epochs, lr in itertools.product(\n",
        "        latent_dim_options, epochs_options, lr_options\n",
        "    ):\n",
        "        print(f\"\\nTestando: latent_dim={latent_dim}, epochs={epochs}, lr={lr}\")\n",
        "\n",
        "        # Instancia o modelo BivaeModel com a configuração atual\n",
        "        model = BivaeModel(\n",
        "            dataset=dataset,\n",
        "            top_k=top_k,\n",
        "            batch_size=batch_size,\n",
        "            latent_dim=latent_dim,\n",
        "            encoder_dims=encoder_dims,\n",
        "            act_func=act_func,\n",
        "            likelihood=likelihood,\n",
        "            epochs=epochs,\n",
        "            lr=lr,\n",
        "            test_size=test_size,\n",
        "            seed=seed\n",
        "        )\n",
        "\n",
        "        # Avalia o modelo\n",
        "        result = model.evaluate()\n",
        "\n",
        "        # Adiciona informações de identificação\n",
        "        result['algorithm'] = 'bivae'\n",
        "        result['version'] = f\"latent_dim_{latent_dim}_epochs_{epochs}_lr_{lr}\"\n",
        "\n",
        "        # Converte o dicionário de resultado em um DataFrame de uma linha e concatena\n",
        "        df_results = pd.concat([df_results, pd.DataFrame([result])], ignore_index=True)\n",
        "\n",
        "        print(f\"Resultado: {result}\")\n",
        "\n",
        "    return df_results\n"
      ],
      "metadata": {
        "id": "VO6C4c6CqQzu"
      },
      "id": "VO6C4c6CqQzu",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "bivae_result = evaluate_bivae()"
      ],
      "metadata": {
        "id": "lh2QVxKLvogc"
      },
      "id": "lh2QVxKLvogc",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "full_results = pd.concat([full_results, bivae_result], ignore_index=True)\n",
        "full_results.to_csv('04_sar_als_ncf_bivae.csv', index=False)"
      ],
      "metadata": {
        "id": "OZW50DFtvpAE"
      },
      "id": "OZW50DFtvpAE",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import itertools\n",
        "import pandas as pd\n",
        "from src.utils.enums import MovieLensDataset\n",
        "from src.light_gcn_model import LightGcnModel\n",
        "\n",
        "def evaluate_lightgcn():\n",
        "    # Parâmetros a serem testados\n",
        "    n_layers_options = [2, 3, 4]\n",
        "    lr_options = [0.005, 0.01]\n",
        "    epochs_options = [1, 5]\n",
        "\n",
        "    # Parâmetros fixos\n",
        "    batch_size = 1024\n",
        "    top_k = 10\n",
        "    test_size = 0.2\n",
        "    seed = 42\n",
        "    dataset = MovieLensDataset.ML_1M\n",
        "\n",
        "    # Cria um DataFrame vazio para armazenar os resultados\n",
        "    df_results = pd.DataFrame()\n",
        "\n",
        "    # Itera por todas as combinações de parâmetros\n",
        "    for n_layers, lr, epochs in itertools.product(n_layers_options, lr_options, epochs_options):\n",
        "        print(f\"\\nTestando: n_layers={n_layers}, lr={lr}, epochs={epochs}\")\n",
        "\n",
        "        # Instancia o modelo LightGcnModel com a configuração atual\n",
        "        model = LightGcnModel(\n",
        "            dataset=dataset,\n",
        "            n_layers=n_layers,\n",
        "            batch_size=batch_size,\n",
        "            lr=lr,\n",
        "            epochs=epochs,\n",
        "            top_k=top_k,\n",
        "            test_size=test_size,\n",
        "            seed=seed\n",
        "        )\n",
        "\n",
        "        # Avalia o modelo\n",
        "        result = model.evaluate()\n",
        "\n",
        "        # Adiciona informações de identificação\n",
        "        result['algorithm'] = 'lightgcn'\n",
        "        result['version'] = f\"n_layers_{n_layers}_lr_{lr}_epochs_{epochs}\"\n",
        "\n",
        "        # Converte o dicionário de resultado em um DataFrame de uma linha e concatena\n",
        "        df_results = pd.concat([df_results, pd.DataFrame([result])], ignore_index=True)\n",
        "\n",
        "        print(f\"Resultado: {result}\")\n",
        "\n",
        "    return df_results\n"
      ],
      "metadata": {
        "id": "Xih-qDW4qbEx"
      },
      "id": "Xih-qDW4qbEx",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "lightgcn_result = evaluate_lightgcn()"
      ],
      "metadata": {
        "id": "gN0D_3ahvvJI"
      },
      "id": "gN0D_3ahvvJI",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "full_results = pd.concat([full_results, lightgcn_result], ignore_index=True)\n",
        "full_results.to_csv('05_sar_als_ncf_bivae_lightgcn.csv', index=False)"
      ],
      "metadata": {
        "id": "0R6MBN0rvu_p"
      },
      "id": "0R6MBN0rvu_p",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import itertools\n",
        "import pandas as pd\n",
        "from src.utils.enums import MovieLensDataset\n",
        "from src.fastai_model import FastAiModel\n",
        "\n",
        "def evaluate_fastai():\n",
        "    # Parâmetros a serem testados\n",
        "    n_factors_options = [40, 50]\n",
        "    epochs_options = [1, 2]\n",
        "\n",
        "    # Parâmetros fixos\n",
        "    test_size = 0.25\n",
        "    top_k = 10\n",
        "    seed = 42\n",
        "    dataset = MovieLensDataset.ML_1M\n",
        "\n",
        "    # Cria um DataFrame vazio para armazenar os resultados\n",
        "    df_results = pd.DataFrame()\n",
        "\n",
        "    # Itera por todas as combinações de parâmetros\n",
        "    for n_factors, epochs in itertools.product(n_factors_options, epochs_options):\n",
        "        print(f\"\\nTestando: n_factors={n_factors}, epochs={epochs}\")\n",
        "\n",
        "        # Instancia o modelo FastAiModel com a configuração atual\n",
        "        model = FastAiModel(\n",
        "            dataset=dataset,\n",
        "            n_factors=n_factors,\n",
        "            test_size=test_size,\n",
        "            epochs=epochs,\n",
        "            top_k=top_k,\n",
        "            seed=seed\n",
        "        )\n",
        "\n",
        "        # Avalia o modelo\n",
        "        result = model.evaluate()\n",
        "\n",
        "        # Adiciona informações de identificação\n",
        "        result['algorithm'] = 'fastai'\n",
        "        result['version'] = f\"n_factors_{n_factors}_epochs_{epochs}\"\n",
        "\n",
        "        # Converte o dicionário de resultado em um DataFrame de uma linha e concatena\n",
        "        df_results = pd.concat([df_results, pd.DataFrame([result])], ignore_index=True)\n",
        "\n",
        "        print(f\"Resultado: {result}\")\n",
        "\n",
        "    return df_results\n"
      ],
      "metadata": {
        "id": "qmaQVlkUqeKn"
      },
      "id": "qmaQVlkUqeKn",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fastai_result = evaluate_fastai()"
      ],
      "metadata": {
        "id": "AfhvDeV7vxM3"
      },
      "id": "AfhvDeV7vxM3",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "full_results = pd.concat([full_results, fastai_result], ignore_index=True)\n",
        "full_results.to_csv('06_sar_als_ncf_bivae_lightgcn_fastai.csv', index=False)"
      ],
      "metadata": {
        "id": "SW91q2BzvxrN"
      },
      "id": "SW91q2BzvxrN",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 2
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython2",
      "version": "2.7.6"
    },
    "colab": {
      "provenance": [],
      "gpuType": "A100",
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}